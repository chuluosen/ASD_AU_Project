"""
Stage 1: HDA-SynChildé¢„è®­ç»ƒè„šæœ¬ - Colabé€‚é…ç‰ˆ
åªè®­ç»ƒYOLOv9ä¸»å¹²+æ£€æµ‹å¤´ï¼Œå†»ç»“GAT-AUå’Œæƒ…ç»ªå¤´
"""

import os
import sys
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from pathlib import Path
import yaml
import logging
from tqdm import tqdm
import numpy as np
from datetime import datetime
import shutil
from google.colab import drive
import gc

# ============= Colabç¯å¢ƒè®¾ç½® =============
def setup_colab_env():
    """è®¾ç½®Colabç¯å¢ƒ"""
    # æŒ‚è½½Google Drive
    if not os.path.exists('/content/drive'):
        print("Mounting Google Drive...")
        drive.mount('/content/drive')
    
    # æ£€æŸ¥GPUå¹¶æ˜¾ç¤ºä¼˜åŒ–ä¿¡æ¯
    if torch.cuda.is_available():
        gpu_info = torch.cuda.get_device_properties(0)
        gpu_name = gpu_info.name
        gpu_memory = gpu_info.total_memory / 1024**3
        
        print(f"ğŸš€ GPU: {gpu_name}")
        print(f"ğŸ’¾ GPU Memory: {gpu_memory:.1f} GB")
        
        # æ˜¾ç¤ºé’ˆå¯¹ç‰¹å®šGPUçš„ä¼˜åŒ–ä¿¡æ¯
        if "A100" in gpu_name:
            if gpu_memory > 70:
                print("ğŸ¯ A100-80GBæ£€æµ‹åˆ°ï¼å·²å¯ç”¨é«˜æ€§èƒ½ä¼˜åŒ–é…ç½®")
                print("   - Batch Size: â†‘ æœ€å¤§48")
                print("   - Workers: â†‘ æœ€å¤§20")
                print("   - TensorFloat-32: âœ… å¯ç”¨")
                print("   - FPSç›®æ ‡: â‰¥80")
            else:
                print("ğŸ¯ A100-40GBæ£€æµ‹åˆ°ï¼å·²å¯ç”¨ä¼˜åŒ–é…ç½®")
                print("   - Batch Size: â†‘ æœ€å¤§32")  
                print("   - Workers: â†‘ æœ€å¤§16")
        elif "V100" in gpu_name:
            print("âš¡ V100æ£€æµ‹åˆ°ï¼å·²å¯ç”¨ä¸­ç­‰ä¼˜åŒ–é…ç½®")
            print("   - Batch Size: â†‘ æœ€å¤§24")
            print("   - FPSç›®æ ‡: â‰¥50")
        elif gpu_memory < 16:
            print("ğŸ’¡ å°æ˜¾å­˜GPUæ£€æµ‹åˆ°ï¼Œå·²å¯ç”¨å†…å­˜å®‰å…¨é…ç½®")
            print("   - Batch Size: é™åˆ¶ä¸º8")
        
        print(f"ğŸ”§ CUDA Version: {torch.version.cuda}")
        print(f"ğŸ”§ PyTorch Version: {torch.__version__}")
    else:
        raise RuntimeError("No GPU available in Colab!")
    
    # è®¾ç½®é¡¹ç›®è·¯å¾„
    project_root = Path('/content/ASD_AU_Project')
    if not project_root.exists():
        raise RuntimeError(f"Project not found at {project_root}. Please clone the repository first!")
    
    # æ·»åŠ è·¯å¾„
    sys.path.append(str(project_root))
    sys.path.append(str(project_root / 'code' / 'yolov9'))
    
    # åˆ›å»ºå¿…è¦çš„ç›®å½•
    drive_save_dir = Path('/content/drive/MyDrive/ASD_AU_weights/stage1')
    drive_save_dir.mkdir(parents=True, exist_ok=True)
    
    # æ¸…ç†ç¼“å­˜é¿å…æŸåæ–‡ä»¶é—®é¢˜ï¼ˆè½»é‡çº§ï¼‰
    print("Cleaning recent cache files...")
    try:
        # åªæ¸…ç†æ˜ç¡®çŸ¥é“çš„ç¼“å­˜ä½ç½®
        cache_files = [
            '/content/ASD_AU_Project/train.cache',
            '/content/ASD_AU_Project/val.cache',
            '/tmp/train.cache',
            '/tmp/val.cache'
        ]
        cache_count = 0
        for cache_file in cache_files:
            if os.path.exists(cache_file):
                os.remove(cache_file)
                cache_count += 1
        print(f"Cleaned {cache_count} cache files")
    except Exception as e:
        print(f"Cache cleaning skipped: {e}")
    
    # æ£€æŸ¥æ•°æ®åˆ’åˆ†è„šæœ¬çš„è¾“å‡ºï¼ˆæ”¯æŒåµŒå¥—è·¯å¾„ï¼‰
    data_candidates = [
        Path('/content/yolo_data/yolo_data'),  # ç”¨æˆ·å½“å‰çš„å®é™…è·¯å¾„
        Path('/content/yolo_data'),            # æ ‡å‡†è·¯å¾„
    ]
    
    found_data = False
    for yolo_data_path in data_candidates:
        if yolo_data_path.exists() and (yolo_data_path / 'images').exists():
            print(f"âœ… Found dataset at: {yolo_data_path}")
            # æ˜¾ç¤ºæ•°æ®ç»Ÿè®¡
            for split in ['train', 'val', 'test']:
                img_dir = yolo_data_path / 'images' / split
                if img_dir.exists():
                    img_count = len(list(img_dir.glob('*')))
                    print(f"   {split:5s}: {img_count:,} images")
            found_data = True
            break
    
    if not found_data:
        print("âš ï¸  No dataset found in expected locations")
    
    return project_root, drive_save_dir

# è®¾ç½®ç¯å¢ƒ
PROJECT_ROOT, DRIVE_SAVE_DIR = setup_colab_env()

# ç¡®ä¿YOLOv9è·¯å¾„æ­£ç¡®æ·»åŠ 
yolov9_path = PROJECT_ROOT / 'code' / 'yolov9'
if str(yolov9_path) not in sys.path:
    sys.path.insert(0, str(yolov9_path))

# éªŒè¯è·¯å¾„å’Œå¯¼å…¥
print(f"YOLOv9 path: {yolov9_path}")
print(f"YOLOv9 path exists: {yolov9_path.exists()}")

try:
    # å¯¼å…¥YOLOv9ç»„ä»¶
    from models.yolo import Model as Yolov9
    from utils.dataloaders import create_dataloader
    from utils.general import increment_path, colorstr, check_img_size, LOGGER
    from utils.loss_tal import ComputeLoss
    from utils.torch_utils import ModelEMA, de_parallel
    from utils.metrics import fitness
    from val import run as validate
    from utils.callbacks import Callbacks
    print("âœ… YOLOv9 modules imported successfully")
except ImportError as e:
    print(f"âŒ Failed to import YOLOv9 modules: {e}")
    print("ğŸ“ Available files in yolov9 directory:")
    if yolov9_path.exists():
        import os
        for item in os.listdir(yolov9_path):
            print(f"   {item}")
    raise

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from yolo_modules.model_wrapper import Y9_GAT_DA


class ColabStage1Trainer:
    def __init__(self, config):
        self.config = config
        self.device = torch.device(config['device'])
        
        # ä½¿ç”¨ä¸´æ—¶ç›®å½•ä½œä¸ºå·¥ä½œç›®å½•
        self.save_dir = Path(config['save_dir'])
        self.save_dir.mkdir(parents=True, exist_ok=True)
        
        # Driveä¿å­˜ç›®å½•
        self.drive_save_dir = DRIVE_SAVE_DIR
        
        # è®¾ç½®æ—¥å¿—
        self.setup_logging()
        
        # åˆå§‹åŒ–å›è°ƒ
        self.callbacks = Callbacks()
        
        # Colabç‰¹å®šè®¾ç½®
        self.setup_colab_specific()
        
    def setup_colab_specific(self):
        """Colabç‰¹å®šçš„è®¾ç½®"""
        # å‡å°‘æ˜¾å­˜ç¢ç‰‡
        torch.cuda.empty_cache()
        gc.collect()
        
        # è®¾ç½®cudnn
        torch.backends.cudnn.benchmark = True
        
        # A100 TensorFloat-32ä¼˜åŒ–
        if self.config.get('tf32', False):
            torch.backends.cuda.matmul.allow_tf32 = True
            torch.backends.cudnn.allow_tf32 = True
            self.logger.info("Enabled TensorFloat-32 for A100 optimization")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ä¹‹å‰çš„checkpoint
        self.check_resume()
        
    def check_resume(self):
        """æ£€æŸ¥æ˜¯å¦éœ€è¦æ¢å¤è®­ç»ƒ"""
        checkpoint_pattern = self.drive_save_dir / 'checkpoint_epoch_*.pt'
        checkpoints = list(self.drive_save_dir.glob('checkpoint_epoch_*.pt'))
        
        if checkpoints:
            # æ‰¾åˆ°æœ€æ–°çš„checkpoint
            latest_checkpoint = max(checkpoints, key=lambda p: int(p.stem.split('_')[-1]))
            self.logger.info(f"Found checkpoint: {latest_checkpoint}")
            self.config['resume'] = str(latest_checkpoint)
        else:
            self.config['resume'] = None
    
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ"""
        # åŒæ—¶ä¿å­˜åˆ°æœ¬åœ°å’ŒDrive
        log_file = self.save_dir / f"train_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
        drive_log_file = self.drive_save_dir / f"train_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file),
                logging.FileHandler(drive_log_file),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
        
    def create_model(self):
        """åˆ›å»ºYOLOv9æ£€æµ‹å™¨æ¨¡å‹ï¼ˆStage 1 åªè®­ç»ƒæ£€æµ‹å™¨ï¼‰"""
        self.logger.info("Creating YOLOv9 detector for Stage 1...")
        
        # ç›´æ¥åˆ›å»ºYOLOv9æ£€æµ‹å™¨
        cfg_path = self.config['model_cfg']
        
        # å¦‚æœæ˜¯ç›¸å¯¹è·¯å¾„ï¼Œå°è¯•å¤šä¸ªä½ç½®æŸ¥æ‰¾
        if not os.path.isabs(cfg_path):
            cfg_candidates = [
                os.path.join('/content', cfg_path),
                os.path.join(PROJECT_ROOT, 'code', 'yolov9', 'models', 'detect', cfg_path),
                os.path.join(PROJECT_ROOT, 'code', 'yolov9', 'models', cfg_path),
                os.path.join(PROJECT_ROOT, 'code', 'yolov9', 'models', 'detect', 'yolov9-c.yaml'),  # å›é€€
            ]
            
            cfg_found = None
            for candidate in cfg_candidates:
                if os.path.exists(candidate):
                    cfg_found = candidate
                    self.logger.info(f"Found config at: {cfg_found}")
                    break
            
            if cfg_found is None:
                raise FileNotFoundError(f"Config file not found. Tried: {cfg_candidates}")
            cfg_path = cfg_found
        
        # åˆ›å»ºYOLOv9æ¨¡å‹
        model = Yolov9(cfg=cfg_path, ch=3, nc=1)  # nc=1 for face detection
        
        # åŠ è½½é¢„è®­ç»ƒæƒé‡ï¼ˆåœ¨ç§»åˆ°GPUä¹‹å‰ï¼‰
        if self.config['pretrained_weights']:
            weights_path = Path(self.config['pretrained_weights'])
            
            # æŒ‰ä¼˜å…ˆçº§æŸ¥æ‰¾æƒé‡æ–‡ä»¶
            if not weights_path.exists():
                # æ£€æŸ¥å¤šä¸ªå¯èƒ½çš„ä½ç½®
                weight_candidates = [
                    Path('/content') / weights_path.name,                                    # Colabæ ¹ç›®å½•
                    Path('/content/drive/MyDrive/ASD_AU_weights/pretrained') / weights_path.name,  # Driveä½ç½®
                    self.drive_save_dir.parent / 'pretrained' / weights_path.name,          # é»˜è®¤Driveä½ç½®
                    Path('/content') / 'yolov9c_face_weights_only.pt',                      # ä¿®æ­£çš„æƒé‡æ–‡ä»¶ï¼ˆç›´æ¥è·¯å¾„ï¼‰
                ]
                
                for candidate in weight_candidates:
                    if candidate.exists():
                        weights_path = candidate
                        break
            
            if weights_path.exists():
                self.logger.info(f"Loading pretrained weights from {weights_path}")
                
                # æ£€æŸ¥æ˜¯å¦ä¸ºultralyticsæ ¼å¼çš„æƒé‡
                try:
                    # å°è¯•ç›´æ¥åŠ è½½PyTorchæƒé‡
                    ckpt = torch.load(weights_path, map_location=self.device, weights_only=False)
                except Exception as e1:
                    self.logger.warning(f"Direct PyTorch loading failed: {e1}")
                    
                    # å°è¯•ä½¿ç”¨ultralyticsåŠ è½½å¹¶è½¬æ¢
                    try:
                        self.logger.info("Attempting to load ultralytics format and convert...")
                        import subprocess
                        import sys
                        
                        # å®‰è£…ultralyticså¦‚æœæ²¡æœ‰
                        try:
                            import ultralytics
                        except ImportError:
                            self.logger.info("Installing ultralytics...")
                            subprocess.check_call([sys.executable, "-m", "pip", "install", "ultralytics"])
                            import ultralytics
                        
                        from ultralytics import YOLO
                        
                        # åŠ è½½ultralyticsæ¨¡å‹
                        yolo_model = YOLO(str(weights_path))
                        
                        # æå–çº¯PyTorchæƒé‡
                        pure_weights = yolo_model.model.state_dict()
                        
                        # ä¿å­˜è½¬æ¢åçš„æƒé‡
                        converted_path = weights_path.parent / f"{weights_path.stem}_converted.pt"
                        torch.save(pure_weights, converted_path)
                        self.logger.info(f"Converted weights saved to: {converted_path}")
                        
                        # åˆ›å»ºckptå­—å…¸æ ¼å¼
                        ckpt = {'model': pure_weights}
                        
                    except Exception as e2:
                        self.logger.error(f"Ultralytics conversion failed: {e2}")
                        self.logger.warning("Proceeding without pretrained weights...")
                        ckpt = None
                
                # åŠ è½½æ£€æµ‹å™¨æƒé‡ï¼ˆç›´æ¥åŠ è½½åˆ°YOLOv9æ¨¡å‹ï¼‰
                if ckpt is not None:
                    state_dict = ckpt['model'] if 'model' in ckpt else ckpt
                    if hasattr(state_dict, 'state_dict'):
                        state_dict = state_dict.state_dict()
                    
                    # è·å–æ¨¡å‹å½“å‰çš„state_dictä½œä¸ºå‚è€ƒ
                    model_state = model.state_dict()
                    compatible_state = {}
                    
                    # å°è¯•ç›´æ¥åŒ¹é…é”®å
                    for k, v in state_dict.items():
                        # å»æ‰å¯èƒ½çš„detectorå‰ç¼€
                        clean_k = k.replace('detector.', '') if k.startswith('detector.') else k
                        if clean_k in model_state and v.shape == model_state[clean_k].shape:
                            compatible_state[clean_k] = v
                    
                    # åŠ è½½æƒé‡
                    if compatible_state:
                        missing_keys, unexpected_keys = model.load_state_dict(compatible_state, strict=False)
                        self.logger.info(f"Pretrained weights loaded successfully. Matched {len(compatible_state)} layers.")
                        
                        # è¯¦ç»†çš„åŠ è½½ä¿¡æ¯
                        if missing_keys:
                            self.logger.info(f"Missing keys (will be randomly initialized): {len(missing_keys)}")
                        if unexpected_keys:
                            self.logger.info(f"Unexpected keys (ignored): {len(unexpected_keys)}")
                    else:
                        self.logger.warning("No compatible weights found! Training from scratch...")
                        
                    # æ˜¾ç¤ºåŒ¹é…ç»Ÿè®¡
                    total_model_params = len(model_state.keys())
                    self.logger.info(f"Weight loading: {len(compatible_state)}/{total_model_params} layers matched")
                else:
                    self.logger.warning("No pretrained weights loaded, training from scratch")
            else:
                self.logger.warning(f"Pretrained weights not found: {weights_path}")
        
        # æƒé‡åŠ è½½å®Œæˆåï¼Œç§»åŠ¨æ¨¡å‹åˆ°GPU
        model = model.to(self.device)
        self.logger.info(f"Model moved to device: {self.device}")
        
        # æ¢å¤è®­ç»ƒï¼ˆè·³è¿‡ä¸å…¼å®¹çš„checkpointï¼‰
        start_epoch = 0
        if self.config.get('resume'):
            self.logger.info(f"Found checkpoint: {self.config['resume']}")
            try:
                checkpoint = torch.load(self.config['resume'], map_location=self.device)
                # æ£€æŸ¥checkpointæ˜¯å¦ä¸å½“å‰æ¨¡å‹æ¶æ„å…¼å®¹
                checkpoint_keys = set(checkpoint['model_state_dict'].keys())
                model_keys = set(model.state_dict().keys())
                
                # å¦‚æœcheckpointåŒ…å«GAT/emotionç»„ä»¶ï¼Œè¯´æ˜æ˜¯æ—§çš„åŒ…è£…æ¨¡å‹ï¼Œè·³è¿‡
                has_old_components = any(key.startswith(('gat_head', 'emotion_head', 'roi_extractor')) 
                                       for key in checkpoint_keys)
                
                if has_old_components:
                    self.logger.warning("Checkpoint contains old model architecture (Y9_GAT_DA). Starting fresh training with new YOLOv9-only architecture.")
                    start_epoch = 0
                else:
                    # å…¼å®¹çš„checkpointï¼Œå¯ä»¥æ¢å¤
                    model.load_state_dict(checkpoint['model_state_dict'], strict=False)
                    start_epoch = checkpoint['epoch'] + 1
                    self.logger.info(f"Resumed from epoch {start_epoch}")
                    
            except Exception as e:
                self.logger.warning(f"Failed to load checkpoint: {e}. Starting fresh training.")
                start_epoch = 0
        
        # Stage 1ï¼šçº¯YOLOv9æ£€æµ‹å™¨ï¼Œæ— éœ€å†»ç»“æ¨¡å—
        # æ‰“å°å‚æ•°ç»Ÿè®¡
        self.print_param_stats(model)
        
        return model, start_epoch
    
    # freeze_modules å‡½æ•°å·²ç§»é™¤ - Stage1åªè®­ç»ƒçº¯YOLOv9æ£€æµ‹å™¨
        
    def print_param_stats(self, model):
        """æ‰“å°å‚æ•°ç»Ÿè®¡"""
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        
        self.logger.info(f"Total parameters: {total_params:,}")
        self.logger.info(f"Trainable parameters: {trainable_params:,}")
        self.logger.info(f"Frozen parameters: {total_params - trainable_params:,}")
        
    def setup_data_paths(self):
        """è®¾ç½®æ•°æ®è·¯å¾„ï¼ˆå…¼å®¹ç”¨æˆ·çš„æ•°æ®åˆ’åˆ†è„šæœ¬ï¼‰"""
        # æŒ‰ä¼˜å…ˆçº§æ£€æŸ¥æ•°æ®è·¯å¾„
        data_candidates = [
            Path('/content/yolo_data/yolo_data'),                         # ç”¨æˆ·å½“å‰çš„å®é™…è·¯å¾„
            Path('/content/yolo_data'),                                    # ç”¨æˆ·åˆ’åˆ†è„šæœ¬çš„æ ‡å‡†è¾“å‡º
            Path('/content/drive/MyDrive/datasets/HDA-SynChild'),         # Driveæ ‡å‡†ä½ç½®
            Path('/content/datasets/HDA-SynChild'),                       # æœ¬åœ°ä½ç½®
        ]
        
        for data_root in data_candidates:
            if data_root.exists() and (data_root / 'images' / 'train').exists():
                self.logger.info(f"Found dataset at: {data_root}")
                
                # å¦‚æœæ•°æ®åœ¨åµŒå¥—ä½ç½®ï¼Œåˆ›å»ºç¬¦å·é“¾æ¥åˆ°æ ‡å‡†ä½ç½®ä¼˜åŒ–è®¿é—®
                if str(data_root) in ['/content/yolo_data', '/content/yolo_data/yolo_data']:
                    standard_path = Path('/content/datasets/HDA-SynChild')
                    standard_path.parent.mkdir(exist_ok=True)
                    if not standard_path.exists():
                        os.symlink(data_root, standard_path)
                        self.logger.info(f"Created symlink: {standard_path} -> {data_root}")
                        return standard_path
                
                return data_root
        
        # å¦‚æœéƒ½æ²¡æ‰¾åˆ°ï¼Œç»™å‡ºè¯¦ç»†çš„æç¤º
        raise RuntimeError(
            "Dataset not found! Please ensure your data is in one of these locations:\n"
            "1. /content/yolo_data (output from your data split script)\n"
            "2. /content/drive/MyDrive/datasets/HDA-SynChild\n"
            "3. /content/datasets/HDA-SynChild\n"
            "Required structure: {path}/images/train/, {path}/images/val/, {path}/labels/train/, {path}/labels/val/"
        )
    
    def check_image_integrity(self, data_root):
        """æ£€æŸ¥å›¾åƒå®Œæ•´æ€§ï¼Œåˆ é™¤æŸåçš„æ–‡ä»¶"""
        if not self.config.get('check_images', False):
            return
            
        print("Checking image integrity...")
        import cv2
        corrupt_count = 0
        
        for split in ['train', 'val']:
            img_dir = data_root / 'images' / split
            if not img_dir.exists():
                continue
                
            img_files = list(img_dir.glob('*.jpg')) + list(img_dir.glob('*.png')) + list(img_dir.glob('*.jpeg'))
            
            for img_path in img_files:
                try:
                    # å°è¯•è¯»å–å›¾åƒ
                    img = cv2.imread(str(img_path))
                    if img is None:
                        print(f"Removing corrupt image: {img_path}")
                        img_path.unlink()  # åˆ é™¤æŸåçš„æ–‡ä»¶
                        
                        # åŒæ—¶åˆ é™¤å¯¹åº”çš„æ ‡ç­¾æ–‡ä»¶
                        label_path = data_root / 'labels' / split / (img_path.stem + '.txt')
                        if label_path.exists():
                            label_path.unlink()
                            
                        corrupt_count += 1
                    else:
                        # æ£€æŸ¥å›¾åƒæ˜¯å¦èƒ½æ­£å¸¸è§£ç 
                        if img.shape[0] < 10 or img.shape[1] < 10:
                            print(f"Removing too small image: {img_path}")
                            img_path.unlink()
                            
                            label_path = data_root / 'labels' / split / (img_path.stem + '.txt')
                            if label_path.exists():
                                label_path.unlink()
                                
                            corrupt_count += 1
                except Exception as e:
                    print(f"Error checking {img_path}: {e}")
                    try:
                        img_path.unlink()
                        label_path = data_root / 'labels' / split / (img_path.stem + '.txt')
                        if label_path.exists():
                            label_path.unlink()
                        corrupt_count += 1
                    except:
                        pass
        
        if corrupt_count > 0:
            print(f"Removed {corrupt_count} corrupt images")
        else:
            print("All images are intact")

    def create_dataloaders(self):
        """åˆ›å»ºæ•°æ®åŠ è½½å™¨ï¼ˆColabä¼˜åŒ–ï¼‰"""
        # è®¾ç½®æ•°æ®è·¯å¾„
        data_root = self.setup_data_paths()
        
        # è·³è¿‡è€—æ—¶çš„å›¾åƒå®Œæ•´æ€§æ£€æŸ¥ï¼ˆå¯åœ¨é¦–æ¬¡è¿è¡Œæ—¶æ‰‹åŠ¨æ£€æŸ¥ï¼‰
        # self.check_image_integrity(data_root)
        
        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨ç”¨æˆ·ç”Ÿæˆçš„data.yaml
        user_data_yaml = data_root / 'data.yaml'
        if user_data_yaml.exists():
            self.logger.info(f"Using existing data.yaml from: {user_data_yaml}")
            data_yaml = user_data_yaml
        else:
            # åˆ›å»ºdata yaml
            data_yaml = self.save_dir / 'hda_synchild_colab.yaml'
            data_dict = {
                'path': str(data_root),
                'train': 'images/train',
                'val': 'images/val',
                'names': {0: 'face'}
            }
            
            with open(data_yaml, 'w') as f:
                yaml.dump(data_dict, f)
            self.logger.info(f"Created data.yaml at: {data_yaml}")
        
        
        # Colabä¼˜åŒ–çš„è¶…å‚æ•°
        hyp = self.get_hyp_dict()
        
        # æ ¹æ®GPUç±»å‹å’Œå†…å­˜è‡ªåŠ¨è°ƒæ•´batch size
        gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1024**3
        gpu_name = torch.cuda.get_device_properties(0).name
        
        if "A100" in gpu_name:
            # A100ä¼˜åŒ–ï¼š80GBæ˜¾å­˜ï¼Œå¯ä»¥æ”¯æŒå¾ˆå¤§çš„batch_size
            if gpu_mem > 70:  # A100-80GB
                self.config['batch_size'] = min(self.config['batch_size'], 48)
                self.config['workers'] = min(self.config['workers'], 20)
            else:  # A100-40GB
                self.config['batch_size'] = min(self.config['batch_size'], 32)
                self.config['workers'] = min(self.config['workers'], 16)
            self.logger.info(f"A100 optimized: batch_size={self.config['batch_size']}, workers={self.config['workers']}")
        elif "V100" in gpu_name:
            # V100ä¼˜åŒ–ï¼š32GBæ˜¾å­˜
            self.config['batch_size'] = min(self.config['batch_size'], 24)
            self.config['workers'] = min(self.config['workers'], 12)
            self.logger.info(f"V100 optimized: batch_size={self.config['batch_size']}")
        elif gpu_mem < 16:  # T4/L4ç­‰å°æ˜¾å­˜GPU
            self.config['batch_size'] = min(self.config['batch_size'], 8)
            self.config['workers'] = min(self.config['workers'], 6)
            self.logger.info(f"Small GPU optimized: batch_size={self.config['batch_size']}")
        
        # è®­ç»ƒæ•°æ®åŠ è½½å™¨ - A100ä¼˜åŒ–ç‰ˆ
        train_loader, dataset = create_dataloader(
            path=str(data_root / 'images/train'),
            imgsz=self.config['img_size'],
            batch_size=self.config['batch_size'],
            stride=32,
            single_cls=True,
            hyp=hyp,
            augment=True,
            cache=False,  # ç¦ç”¨ç¼“å­˜é¿å…æŸåæ–‡ä»¶
            rect=self.config.get('rect', False),
            rank=-1,
            workers=self.config['workers'],
            image_weights=False,
            quad=False,
            prefix='train: ',
            shuffle=True,
            pin_memory=self.config.get('pin_memory', True),
            persistent_workers=self.config.get('persistent_workers', True),
            prefetch_factor=self.config.get('prefetch_factor', 2)
        )
        
        # éªŒè¯æ•°æ®åŠ è½½å™¨ - A100ä¼˜åŒ–ç‰ˆ
        val_loader = create_dataloader(
            path=str(data_root / 'images/val'),
            imgsz=self.config['img_size'],
            batch_size=self.config['batch_size'] * 2,
            stride=32,
            single_cls=True,
            hyp=hyp,
            augment=False,
            cache=False,  # ç¦ç”¨ç¼“å­˜é¿å…æŸåæ–‡ä»¶
            rect=True,  # éªŒè¯æ—¶ä½¿ç”¨rectæé€Ÿ
            rank=-1,
            workers=self.config['workers'],
            pad=0.5,
            prefix='val: ',
            pin_memory=self.config.get('pin_memory', True),
            persistent_workers=self.config.get('persistent_workers', True),
            prefetch_factor=self.config.get('prefetch_factor', 2)
        )[0]
        
        return train_loader, val_loader, dataset
    
    def get_hyp_dict(self):
        """è·å–æ•°æ®å¢å¼ºè¶…å‚æ•°ï¼ˆColabä¼˜åŒ–ï¼‰"""
        return {
            'lr0': self.config['lr0'],
            'lrf': 0.1,
            'momentum': 0.937,
            'weight_decay': 0.0005,
            'warmup_epochs': self.config.get('warmup_epochs', 2),
            'warmup_momentum': 0.8,
            'warmup_bias_lr': 0.1,
            'box': 0.05,
            'cls': 0.5,
            'cls_pw': 1.0,
            'obj': 1.0,
            'obj_pw': 1.0,
            'iou_t': 0.20,
            'anchor_t': 4.0,
            'anchors': 3,
            'fl_gamma': 0.0,
            'hsv_h': 0.015,
            'hsv_s': 0.7,
            'hsv_v': 0.4,
            'degrees': 0.0,
            'translate': 0.1,
            'scale': 0.5,
            'shear': 0.0,
            'perspective': 0.0,
            'flipud': 0.0,
            'fliplr': 0.5,
            'mosaic': 0.5,  # é™ä½ä»¥èŠ‚çœå†…å­˜
            'mixup': 0.0,
            'copy_paste': 0.0,
            'paste_in': 0.15,
            'loss_ota': 1,
        }
    
    def save_checkpoint(self, model, optimizer, epoch, best_fitness, ema=None, best=False):
        """ä¿å­˜æ£€æŸ¥ç‚¹åˆ°Google Drive"""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'best_fitness': best_fitness,
            'config': self.config
        }
        
        if ema:
            checkpoint['ema_state_dict'] = ema.ema.state_dict()
        
        # ä¿å­˜åˆ°æœ¬åœ°
        if best:
            local_path = self.save_dir / 'weights' / 'best.pt'
        else:
            local_path = self.save_dir / 'weights' / f'epoch_{epoch}.pt'
        
        local_path.parent.mkdir(exist_ok=True)
        torch.save(checkpoint, local_path)
        
        # å¤åˆ¶åˆ°Drive
        if best:
            drive_path = self.drive_save_dir / 'best.pt'
        else:
            drive_path = self.drive_save_dir / f'checkpoint_epoch_{epoch}.pt'
            # åªä¿ç•™æœ€è¿‘5ä¸ªcheckpoint
            self.cleanup_old_checkpoints()
        
        shutil.copy2(local_path, drive_path)
        self.logger.info(f"Checkpoint saved to {drive_path}")
    
    def cleanup_old_checkpoints(self):
        """æ¸…ç†æ—§çš„æ£€æŸ¥ç‚¹ï¼Œåªä¿ç•™æœ€è¿‘çš„5ä¸ª"""
        checkpoints = list(self.drive_save_dir.glob('checkpoint_epoch_*.pt'))
        if len(checkpoints) > 5:
            checkpoints.sort(key=lambda p: int(p.stem.split('_')[-1]))
            for ckpt in checkpoints[:-5]:
                ckpt.unlink()
                self.logger.info(f"Removed old checkpoint: {ckpt}")
    
    def train_epoch(self, model, train_loader, optimizer, compute_loss, epoch, epochs, scaler=None):
        """è®­ç»ƒä¸€ä¸ªepochï¼ˆå†…å­˜ä¼˜åŒ–ï¼‰"""
        model.train()
        pbar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f'Epoch {epoch+1}/{epochs}')
        
        losses = []
        accumulation_steps = self.config.get('gradient_accumulation', 1)
        
        for i, (imgs, targets, paths, _) in pbar:
            imgs = imgs.to(self.device, non_blocking=True).float() / 255.0
            
            # å‰å‘ä¼ æ’­
            with torch.autocast(device_type='cuda', dtype=torch.float16, enabled=self.config.get('amp', True)):
                # ç›´æ¥ä½¿ç”¨YOLOv9æ¨¡å‹
                pred = model(imgs)
                
                # ç¬¬ä¸€ä¸ªbatchæ—¶æ‰“å°è°ƒè¯•ä¿¡æ¯
                if i == 0:
                    self.logger.info(f"Model output type: {type(pred)}")
                    if isinstance(pred, list):
                        self.logger.info(f"Output list length: {len(pred)}")
                        for j, p in enumerate(pred):
                            self.logger.info(f"pred[{j}] type: {type(p)}, shape/len: {p.shape if hasattr(p, 'shape') else len(p) if isinstance(p, list) else 'unknown'}")
                
                # Stage1: å¤„ç†æ¨¡å‹è¾“å‡ºæ ¼å¼ - ç¡®ä¿è¾“å‡ºæ˜¯tensoråˆ—è¡¨è€Œä¸æ˜¯åµŒå¥—åˆ—è¡¨
                if isinstance(pred, list) and len(pred) > 0:
                    # æ£€æŸ¥æ˜¯å¦ä¸ºåµŒå¥—åˆ—è¡¨ï¼ˆè®­ç»ƒæ¨¡å¼ä¸‹å¯èƒ½è¿”å› [detection_outputs, aux_outputs]ï¼‰
                    if isinstance(pred[0], list):
                        # å–ç¬¬ä¸€ä¸ªå­åˆ—è¡¨ä½œä¸ºæ£€æµ‹è¾“å‡º
                        pred = pred[0]
                        if i == 0:
                            self.logger.info(f"Using nested list[0] as detection output, length: {len(pred)}")
                    
                    # ç¡®ä¿æ‰€æœ‰å…ƒç´ éƒ½æ˜¯tensor
                    pred = [p for p in pred if hasattr(p, 'view')]  # åªä¿ç•™tensor
                    if i == 0:
                        self.logger.info(f"Final prediction tensors: {len(pred)}")
                
                loss, loss_items = compute_loss(pred, targets.to(self.device))
                loss = loss / accumulation_steps
            
            # åå‘ä¼ æ’­ï¼ˆä½¿ç”¨GradScaleræ”¯æŒAMPï¼‰
            if scaler is not None:
                scaler.scale(loss).backward()
                
                # æ¢¯åº¦ç´¯ç§¯
                if (i + 1) % accumulation_steps == 0:
                    scaler.step(optimizer)
                    scaler.update()
                    optimizer.zero_grad()
            else:
                loss.backward()
                
                # æ¢¯åº¦ç´¯ç§¯
                if (i + 1) % accumulation_steps == 0:
                    optimizer.step()
                    optimizer.zero_grad()
            
            # æ›´æ–°è¿›åº¦æ¡
            losses.append(loss_items.cpu().numpy())
            if len(losses) > 100:
                losses = losses[-100:]  # é™åˆ¶å†…å­˜ä½¿ç”¨
            
            mean_loss = np.mean(losses, axis=0)
            pbar.set_postfix({
                'loss': f'{mean_loss[0]:.4f}',
                'GPU': f'{torch.cuda.memory_reserved() / 1024**3:.1f}G'
            })
            
            # ç§»é™¤é¢‘ç¹çš„æ˜¾å­˜æ¸…ç†ï¼ˆä¼šé™ä½æ€§èƒ½ï¼‰
        
        # å¤„ç†æœ€åä¸€æ‰¹å‰©ä½™çš„æ¢¯åº¦
        if (len(train_loader)) % accumulation_steps != 0:
            if scaler is not None:
                scaler.step(optimizer)
                scaler.update()
            else:
                optimizer.step()
            optimizer.zero_grad()
        
        return np.mean(losses, axis=0)
    
    def train(self):
        """ä¸»è®­ç»ƒå‡½æ•°"""
        # åˆ›å»ºæ¨¡å‹
        model, start_epoch = self.create_model()
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader, val_loader, dataset = self.create_dataloaders()
        
        # æ·»åŠ è¶…å‚æ•°åˆ°æ¨¡å‹ï¼ˆæŸå¤±å‡½æ•°éœ€è¦ï¼‰
        hyp = self.get_hyp_dict()
        model.hyp = hyp
        
        # è®¾ç½®æ¨¡å‹ä¸ºè®­ç»ƒæ¨¡å¼
        model.train()
        
        # ç¡®ä¿æ¨¡å‹strideå’Œæ£€æµ‹å¤´æ­£ç¡®åˆå§‹åŒ–
        self.logger.info("Initializing model detection head...")
        with torch.no_grad():
            _ = model(torch.zeros(1, 3, self.config['img_size'], self.config['img_size'], device=self.device))
        
        # åˆ›å»ºæŸå¤±å‡½æ•°
        compute_loss = ComputeLoss(model)
        
        # åˆ›å»ºä¼˜åŒ–å™¨
        optimizer = self.create_optimizer(model)
        
        # æ¢å¤ä¼˜åŒ–å™¨çŠ¶æ€
        if self.config.get('resume') and start_epoch > 0:
            checkpoint = torch.load(self.config['resume'], map_location=self.device)
            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        
        # åˆ›å»ºè°ƒåº¦å™¨
        scheduler = self.create_scheduler(optimizer, self.config['epochs'] - start_epoch)
        
        # åˆ›å»ºEMAå’Œæ¢¯åº¦ç¼©æ”¾å™¨
        ema = ModelEMA(model) if self.config.get('use_ema', True) else None
        scaler = torch.amp.GradScaler('cuda') if self.config.get('amp', True) else None
        
        # è®­ç»ƒå¾ªç¯
        best_fitness = 0
        best_map50_95 = 0
        patience_counter = 0
        patience = self.config.get('patience', 8)
        
        # è®­ç»ƒæŒ‡æ ‡è®°å½•
        self.train_metrics = {
            'epochs': [],
            'train_loss': [],
            'val_loss': [],
            'mAP@0.5': [],
            'mAP@0.5:0.95': [],
            'precision': [],
            'recall': [],
            'lr': []
        }
        
        try:
            for epoch in range(start_epoch, self.config['epochs']):
                # è®­ç»ƒ
                train_loss = self.train_epoch(model, train_loader, optimizer, compute_loss, 
                                            epoch, self.config['epochs'], scaler)
                
                # éªŒè¯ï¼ˆæŒ‰é¢‘ç‡è¿›è¡Œï¼‰
                val_loss = None
                map_results = None
                
                if (epoch + 1) % self.config.get('val_frequency', 1) == 0:
                    val_loss = self.validate(model, val_loader, compute_loss)
                    
                    # è¯¦ç»†mAPè¯„ä¼°ï¼ˆæ¯5ä¸ªepochæˆ–æœ€åå‡ ä¸ªepochï¼‰
                    if (epoch + 1) % 5 == 0 or epoch >= self.config['epochs'] - 3:
                        map_results = self.evaluate_map(model, val_loader, 
                                                       self.save_dir / 'hda_synchild_colab.yaml')
                
                # æ›´æ–°EMA
                if ema:
                    ema.update(model)
                
                # æ›´æ–°å­¦ä¹ ç‡
                scheduler.step()
                
                # è®°å½•æŒ‡æ ‡
                current_lr = optimizer.param_groups[0]['lr']
                self.train_metrics['epochs'].append(epoch + 1)
                self.train_metrics['train_loss'].append(train_loss[0])
                self.train_metrics['lr'].append(current_lr)
                
                if val_loss is not None:
                    self.train_metrics['val_loss'].append(val_loss[0])
                else:
                    self.train_metrics['val_loss'].append(None)
                
                if map_results:
                    self.train_metrics['mAP@0.5'].append(map_results['mAP@0.5'])
                    self.train_metrics['mAP@0.5:0.95'].append(map_results['mAP@0.5:0.95'])
                    self.train_metrics['precision'].append(map_results['precision'])
                    self.train_metrics['recall'].append(map_results['recall'])
                else:
                    self.train_metrics['mAP@0.5'].append(None)
                    self.train_metrics['mAP@0.5:0.95'].append(None)
                    self.train_metrics['precision'].append(None)
                    self.train_metrics['recall'].append(None)
                
                # è®°å½•æ—¥å¿—
                if val_loss is not None:
                    log_msg = (
                        f"Epoch {epoch+1}/{self.config['epochs']} - "
                        f"Train Loss: {train_loss[0]:.4f} - "
                        f"Val Loss: {val_loss[0]:.4f} - "
                        f"LR: {current_lr:.6f}"
                    )
                    
                    if map_results:
                        log_msg += (
                            f" - mAP@0.5: {map_results['mAP@0.5']:.4f} - "
                            f"mAP@0.5:0.95: {map_results['mAP@0.5:0.95']:.4f} - "
                            f"Precision: {map_results['precision']:.4f} - "
                            f"Recall: {map_results['recall']:.4f}"
                        )
                        
                        # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°ç›®æ ‡mAP@0.5:0.95 >= 0.57
                        if map_results['mAP@0.5:0.95'] >= 0.57:
                            self.logger.info(f"ğŸ¯ Target mAP@0.5:0.95 >= 0.57 achieved: {map_results['mAP@0.5:0.95']:.4f}")
                    
                    log_msg += f" - Patience: {patience_counter}/{patience}"
                    self.logger.info(log_msg)
                    
                    # ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ˆåŸºäºmAP@0.5:0.95æˆ–éªŒè¯æŸå¤±ï¼‰
                    if map_results and map_results['mAP@0.5:0.95'] > best_map50_95:
                        best_map50_95 = map_results['mAP@0.5:0.95']
                        best_fitness = map_results['mAP@0.5:0.95']
                        patience_counter = 0
                        self.save_checkpoint(model, optimizer, epoch, best_fitness, ema, best=True)
                        self.logger.info(f"New best model! mAP@0.5:0.95: {best_map50_95:.4f}")
                    else:
                        # å›é€€åˆ°åŸºäºéªŒè¯æŸå¤±çš„fitness
                        fitness = 1 / (val_loss[0] + 1e-6)
                        if fitness > best_fitness and not map_results:
                            best_fitness = fitness
                            patience_counter = 0
                            self.save_checkpoint(model, optimizer, epoch, best_fitness, ema, best=True)
                            self.logger.info(f"New best model! Fitness: {fitness:.6f}")
                        else:
                            patience_counter += 1
                else:
                    self.logger.info(
                        f"Epoch {epoch+1}/{self.config['epochs']} - "
                        f"Train Loss: {train_loss[0]:.4f} - "
                        f"LR: {current_lr:.6f}"
                    )
                
                # ä¿å­˜æ£€æŸ¥ç‚¹
                if (epoch + 1) % self.config['save_period'] == 0:
                    self.save_checkpoint(model, optimizer, epoch, best_fitness, ema)
                    
                # æ—©åœæ£€æŸ¥
                if patience_counter >= patience:
                    self.logger.info(f"Early stopping triggered at epoch {epoch+1}. "
                                   f"No improvement for {patience} epochs.")
                    self.save_checkpoint(model, optimizer, epoch, best_fitness, ema)
                    break
                
                # æ£€æŸ¥Colabè¿è¡Œæ—¶é—´
                if self.check_runtime_limit():
                    self.logger.warning("Approaching Colab runtime limit. Saving checkpoint...")
                    self.save_checkpoint(model, optimizer, epoch, best_fitness, ema)
                    break
                    
        except Exception as e:
            self.logger.error(f"Training interrupted: {e}")
            self.save_checkpoint(model, optimizer, epoch, best_fitness, ema)
            raise
        
        self.logger.info("Training completed!")
        
        # æœ€ç»ˆè¯„ä¼°å¥—ä»¶
        self.logger.info("ğŸ” å¼€å§‹æœ€ç»ˆæ¨¡å‹è¯„ä¼°...")
        
        # 1. FPSåŸºå‡†æµ‹è¯•
        self.logger.info("ğŸš€ FPSåŸºå‡†æµ‹è¯•...")
        try:
            fps_results = self.benchmark_fps(model)
            
            # ä¿å­˜FPSç»“æœåˆ°Drive
            import json
            fps_file = self.drive_save_dir / 'fps_benchmark.json'
            with open(fps_file, 'w') as f:
                json.dump(fps_results, f, indent=2)
            self.logger.info(f"FPSæµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {fps_file}")
            
        except Exception as e:
            self.logger.warning(f"FPSåŸºå‡†æµ‹è¯•å¤±è´¥: {e}")
        
        # 2. å®šæ€§æ£€æµ‹å¯è§†åŒ–
        self.logger.info("ğŸ“¸ å®šæ€§æ£€æµ‹å¯è§†åŒ–...")
        try:
            qualitative_results = self.qualitative_check(model, val_loader, num_samples=30)
            
            if qualitative_results:
                # ä¿å­˜å®šæ€§æ£€æŸ¥ç»“æœ
                qual_file = self.drive_save_dir / 'qualitative_check.json'
                with open(qual_file, 'w') as f:
                    json.dump(qualitative_results, f, indent=2)
                self.logger.info(f"å®šæ€§æ£€æŸ¥ç»“æœå·²ä¿å­˜åˆ°: {qual_file}")
            
        except Exception as e:
            self.logger.warning(f"å®šæ€§æ£€æŸ¥å¤±è´¥: {e}")
        
        # 3. æœ€ç»ˆmAPè¯„ä¼°
        self.logger.info("ğŸ“Š æœ€ç»ˆmAPè¯„ä¼°...")
        try:
            final_map_results = self.evaluate_map(model, val_loader, 
                                                 self.save_dir / 'hda_synchild_colab.yaml')
            if final_map_results:
                # ä¿å­˜æœ€ç»ˆmAPç»“æœ
                map_file = self.drive_save_dir / 'final_map_results.json'
                with open(map_file, 'w') as f:
                    json.dump(final_map_results, f, indent=2)
                
                self.logger.info("ğŸ¯ æœ€ç»ˆè¯„ä¼°ç»“æœ:")
                self.logger.info(f"   mAP@0.5: {final_map_results['mAP@0.5']:.4f}")
                self.logger.info(f"   mAP@0.5:0.95: {final_map_results['mAP@0.5:0.95']:.4f}")
                self.logger.info(f"   Precision: {final_map_results['precision']:.4f}")
                self.logger.info(f"   Recall: {final_map_results['recall']:.4f}")
                
                # æ€»ç»“è¯„ä¼°çŠ¶æ€
                target_achieved = final_map_results['mAP@0.5:0.95'] >= 0.57
                self.logger.info(f"   ç›®æ ‡è¾¾æˆ: {'âœ… æ˜¯' if target_achieved else 'âŒ å¦'} (â‰¥0.57)")
                
        except Exception as e:
            self.logger.warning(f"æœ€ç»ˆmAPè¯„ä¼°å¤±è´¥: {e}")
        
        return str(self.drive_save_dir / 'best.pt')
    
    def check_runtime_limit(self):
        """æ£€æŸ¥æ˜¯å¦æ¥è¿‘Colabè¿è¡Œæ—¶é™åˆ¶"""
        # è¿™é‡Œå¯ä»¥å®ç°æ›´å¤æ‚çš„é€»è¾‘
        # ç®€å•èµ·è§ï¼Œè®­ç»ƒè¶…è¿‡11å°æ—¶å°±åœæ­¢
        import time
        if hasattr(self, 'start_time'):
            elapsed = time.time() - self.start_time
            return elapsed > 11 * 3600  # 11å°æ—¶
        return False
    
    def create_optimizer(self, model):
        """åˆ›å»ºä¼˜åŒ–å™¨"""
        params_to_optimize = [p for p in model.parameters() if p.requires_grad]
        
        if self.config['optimizer'] == 'AdamW':
            optimizer = torch.optim.AdamW(
                params_to_optimize,
                lr=self.config['lr0'],
                betas=(0.937, 0.999),
                weight_decay=0.0005
            )
        else:
            optimizer = torch.optim.SGD(
                params_to_optimize,
                lr=self.config['lr0'],
                momentum=0.937,
                weight_decay=0.0005,
                nesterov=True
            )
        
        return optimizer
    
    def create_scheduler(self, optimizer, epochs):
        """åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆæ”¯æŒwarmupï¼‰"""
        warmup_epochs = self.config.get('warmup_epochs', 0)
        
        if warmup_epochs > 0 and warmup_epochs < epochs:
            # ä½¿ç”¨warmup + ä¸»è°ƒåº¦å™¨
            from torch.optim.lr_scheduler import LinearLR, SequentialLR
            
            # Warmupé˜¶æ®µï¼šä»10%å­¦ä¹ ç‡çº¿æ€§å¢åŠ åˆ°100%
            warmup_scheduler = LinearLR(
                optimizer, 
                start_factor=0.1, 
                total_iters=warmup_epochs
            )
            
            # ä¸»è®­ç»ƒé˜¶æ®µè°ƒåº¦å™¨
            if self.config['scheduler'] == 'cosine':
                main_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
                    optimizer,
                    T_max=epochs - warmup_epochs,
                    eta_min=self.config['lr0'] * 0.1
                )
            else:
                lambda_lr = lambda epoch: (1 - epoch / (epochs - warmup_epochs)) * (1 - 0.1) + 0.1
                main_scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_lr)
            
            # ç»„åˆè°ƒåº¦å™¨
            scheduler = SequentialLR(
                optimizer, 
                schedulers=[warmup_scheduler, main_scheduler], 
                milestones=[warmup_epochs]
            )
            
            self.logger.info(f"Using warmup scheduler: {warmup_epochs} epochs warmup + {self.config['scheduler']}")
            
        else:
            # ä¸ä½¿ç”¨warmupï¼Œç›´æ¥ä½¿ç”¨ä¸»è°ƒåº¦å™¨
            if self.config['scheduler'] == 'cosine':
                scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
                    optimizer,
                    T_max=epochs,
                    eta_min=self.config['lr0'] * 0.1
                )
            else:
                lambda_lr = lambda epoch: (1 - epoch / epochs) * (1 - 0.1) + 0.1
                scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_lr)
        
        return scheduler
    
    def validate(self, model, val_loader, compute_loss):
        """éªŒè¯æ¨¡å‹ï¼ˆåŒ…å«è¯¦ç»†mAPè¯„ä¼°ï¼‰"""
        model.eval()
        losses = []
        
        with torch.no_grad():
            for imgs, targets, paths, _ in tqdm(val_loader, desc='Validating'):
                imgs = imgs.to(self.device, non_blocking=True).float() / 255.0
                
                with torch.autocast(device_type='cuda', dtype=torch.float16, enabled=self.config.get('amp', True)):
                    pred = model(imgs)
                    loss, loss_items = compute_loss(pred, targets.to(self.device))
                
                losses.append(loss_items.cpu().numpy())
        
        return np.mean(losses, axis=0)
    
    def evaluate_map(self, model, val_loader, data_yaml_path):
        """ä½¿ç”¨YOLOv9çš„val.pyè¿›è¡Œå®Œæ•´mAPè¯„ä¼°"""
        try:
            # ä¿å­˜å½“å‰æ¨¡å‹æƒé‡åˆ°ä¸´æ—¶æ–‡ä»¶ï¼ˆYOLOv9å…¼å®¹æ ¼å¼ï¼‰
            temp_weights = self.save_dir / 'temp_weights.pt'
            torch.save({'model': model.state_dict()}, temp_weights)
            
            # è°ƒç”¨YOLOv9çš„éªŒè¯å‡½æ•°
            from val import run as validate_yolo
            
            results = validate_yolo(
                data=data_yaml_path,
                weights=str(temp_weights),
                batch_size=self.config['batch_size'] * 2,
                imgsz=self.config['img_size'],
                conf_thres=0.001,
                iou_thres=0.6,
                max_det=300,
                task='val',
                device=str(self.device),
                workers=self.config['workers'],
                single_cls=True,
                augment=False,
                verbose=False,
                save_txt=False,
                save_hybrid=False,
                save_conf=False,
                save_json=False,
                project=str(self.save_dir),
                name='val',
                exist_ok=True,
                half=self.config.get('amp', True),
                dnn=False
            )
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if temp_weights.exists():
                temp_weights.unlink()
            
            # æå–å…³é”®æŒ‡æ ‡
            precision, recall, map50, map50_95 = results[:4]
            
            return {
                'precision': precision,
                'recall': recall, 
                'mAP@0.5': map50,
                'mAP@0.5:0.95': map50_95
            }
            
        except Exception as e:
            self.logger.warning(f"mAP evaluation failed: {e}")
            return None
    
    def benchmark_fps(self, model, warmup_runs=10, test_runs=100):
        """FPSåŸºå‡†æµ‹è¯• - ç›®æ ‡: RTX 3060 â‰¥ 28 FPS"""
        model.eval()
        
        # åˆ›å»ºæµ‹è¯•è¾“å…¥
        test_input = torch.randn(1, 3, self.config['img_size'], self.config['img_size'], device=self.device)
        
        # é¢„çƒ­GPU
        with torch.no_grad():
            for _ in range(warmup_runs):
                _ = model(test_input)
        
        # åŒæ­¥CUDAç¡®ä¿é¢„çƒ­å®Œæˆ
        torch.cuda.synchronize()
        
        # FPSæµ‹è¯•
        import time
        start_time = time.time()
        
        with torch.no_grad():
            for _ in range(test_runs):
                _ = model(test_input)
        
        torch.cuda.synchronize()
        end_time = time.time()
        
        # è®¡ç®—FPS
        total_time = end_time - start_time
        fps = test_runs / total_time
        avg_inference_time = total_time / test_runs * 1000  # ms
        
        # GPUä¿¡æ¯
        gpu_name = torch.cuda.get_device_properties(0).name
        
        self.logger.info(f"ğŸš€ FPSåŸºå‡†æµ‹è¯•ç»“æœ:")
        self.logger.info(f"   GPU: {gpu_name}")
        self.logger.info(f"   è¾“å…¥å°ºå¯¸: {self.config['img_size']}x{self.config['img_size']}")
        self.logger.info(f"   æ¨ç†é€Ÿåº¦: {fps:.1f} FPS")
        self.logger.info(f"   å¹³å‡å»¶è¿Ÿ: {avg_inference_time:.2f} ms")
        
        # æ ¹æ®GPUç±»å‹æ£€æŸ¥æ€§èƒ½ç›®æ ‡
        if "A100" in gpu_name:
            target_fps = 80  # A100ç›®æ ‡ï¼šâ‰¥80 FPS
            if fps >= target_fps:
                self.logger.info(f"ğŸš€ A100æ€§èƒ½ä¼˜ç§€! ({fps:.1f} â‰¥ {target_fps} FPS)")
            else:
                self.logger.warning(f"âš ï¸ A100æœªè¾¾ç›®æ ‡! ({fps:.1f} < {target_fps} FPS)")
        elif "V100" in gpu_name:
            target_fps = 50  # V100ç›®æ ‡ï¼šâ‰¥50 FPS
            if fps >= target_fps:
                self.logger.info(f"âœ… V100è¾¾åˆ°ç›®æ ‡! ({fps:.1f} â‰¥ {target_fps} FPS)")
            else:
                self.logger.warning(f"âš ï¸ V100æœªè¾¾ç›®æ ‡! ({fps:.1f} < {target_fps} FPS)")
        elif "RTX 30" in gpu_name or "RTX 40" in gpu_name:
            target_fps = 28
            if fps >= target_fps:
                self.logger.info(f"âœ… è¾¾åˆ°ç›®æ ‡æ€§èƒ½! ({fps:.1f} â‰¥ {target_fps} FPS)")
            else:
                self.logger.warning(f"âš ï¸ æœªè¾¾ç›®æ ‡æ€§èƒ½! ({fps:.1f} < {target_fps} FPS)")
        
        return {
            'fps': fps,
            'avg_inference_time_ms': avg_inference_time,
            'gpu_name': gpu_name,
            'input_size': self.config['img_size']
        }
    
    def qualitative_check(self, model, val_loader, num_samples=50):
        """å®šæ€§æ£€æµ‹å¯è§†åŒ– - éšæœºæŠ½å–éªŒè¯å›¾åƒè¿›è¡Œæ£€æµ‹å¯è§†åŒ–"""
        try:
            import cv2
            import random
            from pathlib import Path
            
            model.eval()
            sample_dir = self.save_dir / 'qualitative_samples'
            sample_dir.mkdir(exist_ok=True)
            
            # éšæœºé‡‡æ ·éªŒè¯å›¾åƒ
            all_samples = []
            for batch_idx, (imgs, targets, paths, _) in enumerate(val_loader):
                for i in range(len(paths)):
                    all_samples.append((imgs[i], paths[i]))  # åªä¿ç•™å›¾åƒå’Œè·¯å¾„ï¼Œç§»é™¤targetsé¿å…ç´¢å¼•é—®é¢˜
                if len(all_samples) >= num_samples * 2:  # é‡‡æ ·æ›´å¤šä»¥ä¾¿éšæœºé€‰æ‹©
                    break
            
            if len(all_samples) < 10:
                self.logger.warning("æ ·æœ¬æ•°é‡ä¸è¶³ï¼Œè·³è¿‡å®šæ€§æ£€æŸ¥")
                return
            
            # éšæœºé€‰æ‹©æ ·æœ¬
            selected_samples = random.sample(all_samples, min(num_samples, len(all_samples)))
            
            detection_count = 0
            good_detections = 0
            
            with torch.no_grad():
                for idx, (img_tensor, img_path) in enumerate(selected_samples):
                    try:
                        # å‡†å¤‡è¾“å…¥
                        img_input = img_tensor.unsqueeze(0).to(self.device).float() / 255.0
                        
                        # æ¨ç†
                        pred = model(img_input)
                        
                        # è¯»å–åŸå›¾
                        img_cv = cv2.imread(str(img_path))
                        if img_cv is None:
                            continue
                        
                        img_h, img_w = img_cv.shape[:2]
                        
                        # ç®€å•çš„æ£€æµ‹ç»“æœå¤„ç†ï¼ˆè¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥ç”¨NMSï¼‰
                        if isinstance(pred, list) and len(pred) > 0:
                            # å–ç¬¬ä¸€ä¸ªå°ºåº¦çš„é¢„æµ‹
                            if isinstance(pred[0], list):
                                first_pred = pred[0][0] if len(pred[0]) > 0 else None
                            else:
                                first_pred = pred[0]
                            
                            if first_pred is not None and len(first_pred.shape) >= 2:
                                detection_count += 1
                                
                                # ç®€åŒ–çš„æ¡†ç»˜åˆ¶ï¼ˆä»…ç”¨äºå®šæ€§æ£€æŸ¥ï¼‰
                                pred_np = first_pred.cpu().numpy()
                                if pred_np.shape[-1] >= 5:  # è‡³å°‘æœ‰x,y,w,h,conf
                                    # æ‰¾ç½®ä¿¡åº¦è¾ƒé«˜çš„æ£€æµ‹æ¡†
                                    conf_scores = pred_np[..., 4] if pred_np.shape[-1] > 4 else pred_np[..., 0]
                                    high_conf_mask = conf_scores > 0.3
                                    
                                    if high_conf_mask.any():
                                        good_detections += 1
                                        # åœ¨å›¾åƒä¸Šæ ‡è®° "Good Detection"
                                        cv2.putText(img_cv, f"Sample {idx+1}: Good Detection", 
                                                  (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
                                    else:
                                        cv2.putText(img_cv, f"Sample {idx+1}: Low Confidence", 
                                                  (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 165, 255), 2)
                                else:
                                    cv2.putText(img_cv, f"Sample {idx+1}: No Detection", 
                                              (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
                        else:
                            cv2.putText(img_cv, f"Sample {idx+1}: No Output", 
                                      (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (128, 128, 128), 2)
                        
                        # ä¿å­˜æ ‡æ³¨å›¾åƒ
                        save_path = sample_dir / f"sample_{idx+1:03d}.jpg"
                        cv2.imwrite(str(save_path), img_cv)
                        
                    except Exception as e:
                        self.logger.warning(f"å¤„ç†æ ·æœ¬ {idx+1} æ—¶å‡ºé”™: {e}")
                        continue
            
            # ç»Ÿè®¡ç»“æœ
            detection_rate = good_detections / len(selected_samples) * 100 if selected_samples else 0
            
            self.logger.info(f"ğŸ“¸ å®šæ€§æ£€æµ‹åˆ†æå®Œæˆ:")
            self.logger.info(f"   æ£€æŸ¥æ ·æœ¬æ•°: {len(selected_samples)}")
            self.logger.info(f"   æœ‰æ•ˆæ£€æµ‹æ•°: {good_detections}")
            self.logger.info(f"   æ£€æµ‹æˆåŠŸç‡: {detection_rate:.1f}%")
            self.logger.info(f"   æ ·æœ¬å›¾ç‰‡ä¿å­˜è‡³: {sample_dir}")
            
            if detection_rate < 30:
                self.logger.warning("âš ï¸ æ£€æµ‹æˆåŠŸç‡è¾ƒä½ï¼Œå»ºè®®æ£€æŸ¥æ¨¡å‹è®­ç»ƒ")
            elif detection_rate > 70:
                self.logger.info("âœ… æ£€æµ‹æ•ˆæœè‰¯å¥½")
            else:
                self.logger.info("ğŸ“Š æ£€æµ‹æ•ˆæœä¸­ç­‰ï¼Œå¯è€ƒè™‘ç»§ç»­è®­ç»ƒ")
                
            return {
                'total_samples': len(selected_samples),
                'good_detections': good_detections,
                'detection_rate': detection_rate,
                'sample_dir': str(sample_dir)
            }
            
        except Exception as e:
            self.logger.warning(f"å®šæ€§æ£€æŸ¥å¤±è´¥: {e}")
            return None


def main():
    """ä¸»å‡½æ•° - Colabç‰ˆæœ¬"""
    import time
    
    config = {
        # æ¨¡å‹é…ç½®
        'model_cfg': 'yolov9c-face-exact.yaml',  # ä¼˜å…ˆä½¿ç”¨ä¿®æ­£çš„é…ç½®ï¼Œå›é€€åˆ°æ ‡å‡†é…ç½®
        'pretrained_weights': 'yolov9c_face_weights_only.pt',  # ä¼šè‡ªåŠ¨åœ¨å¤šä¸ªä½ç½®æŸ¥æ‰¾
        'num_au': 32,
        'num_emotion': 6,
        
        # æ•°æ®é…ç½® - A100 GPUä¼˜åŒ–ï¼ˆ80GBæ˜¾å­˜ç‰ˆï¼‰
        'img_size': 640,
        'batch_size': 32,           # A100å¯ä»¥æ”¯æŒæ›´å¤§batch_size
        'workers': 16,              # A100æ”¯æŒæ›´å¤šå¹¶è¡Œworkers
        'pin_memory': True,         # å¯ç”¨pin_memoryåŠ é€Ÿæ•°æ®ä¼ è¾“
        'cache_images': False,      # ç¦ç”¨ç¼“å­˜é¿å…æŸåæ–‡ä»¶
        'check_images': False,      # è·³è¿‡å›¾åƒæ£€æŸ¥åŠ é€Ÿå¯åŠ¨
        'rect': True,               # A100å†…å­˜å……è¶³ï¼Œå¯ç”¨çŸ©å½¢è®­ç»ƒææ•ˆ
        'prefetch_factor': 4,       # é¢„å–æ›´å¤šæ‰¹æ¬¡
        'persistent_workers': True, # ä¿æŒworkerè¿›ç¨‹æé€Ÿ
        
        # è®­ç»ƒé…ç½® - A100åŠ é€Ÿç‰ˆ
        'epochs': 20,               # ä¿æŒ20ä¸ªepochs
        'lr0': 0.005,               # æé«˜å­¦ä¹ ç‡é€‚é…æ›´å¤§batch_size
        'warmup_epochs': 2,         # warmup epochs
        'optimizer': 'AdamW',
        'scheduler': 'cosine',
        'device': 'cuda:0',
        'use_ema': True,
        'save_period': 5,           # ä¿å­˜é¢‘ç‡
        'amp': True,                # æ··åˆç²¾åº¦è®­ç»ƒ
        'tf32': True,               # å¯ç”¨TensorFloat-32ä¼˜åŒ–
        'gradient_accumulation': 1, # A100å¤§batch_sizeï¼Œå‡å°‘æ¢¯åº¦ç´¯ç§¯
        'val_frequency': 2,         # æ¯2ä¸ªepochéªŒè¯ä¸€æ¬¡ï¼ˆæ›´é¢‘ç¹ï¼‰
        'patience': 8,              # ç¨å¾®å¢åŠ æ—©åœè€å¿ƒ
        
        # ä¿å­˜é…ç½®
        'save_dir': '/content/runs/stage1_hda_synchild_l4',
    }
    
    # è®°å½•å¼€å§‹æ—¶é—´
    start_time = time.time()
    
    # åˆ›å»ºè®­ç»ƒå™¨å¹¶å¼€å§‹è®­ç»ƒ
    trainer = ColabStage1Trainer(config)
    trainer.start_time = start_time
    
    best_weights = trainer.train()
    
    print(f"Training completed! Best weights saved at: {best_weights}")
    print(f"Total time: {(time.time() - start_time) / 3600:.2f} hours")
    

if __name__ == '__main__':
    main()