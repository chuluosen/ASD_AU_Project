"""
Stage 1: HDA-SynChildé¢„è®­ç»ƒè„šæœ¬ - Colabé€‚é…ç‰ˆ
åªè®­ç»ƒYOLOv9ä¸»å¹²+æ£€æµ‹å¤´ï¼Œå†»ç»“GAT-AUå’Œæƒ…ç»ªå¤´
"""

import os
import sys
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from pathlib import Path
import yaml
import logging
from tqdm import tqdm
import numpy as np
from datetime import datetime
import shutil
from google.colab import drive
import gc

# ============= Colabç¯å¢ƒè®¾ç½® =============
def setup_colab_env():
    """è®¾ç½®Colabç¯å¢ƒ"""
    # æŒ‚è½½Google Drive
    if not os.path.exists('/content/drive'):
        print("Mounting Google Drive...")
        drive.mount('/content/drive')
    
    # æ£€æŸ¥GPU
    if torch.cuda.is_available():
        gpu_info = torch.cuda.get_device_properties(0)
        print(f"GPU: {gpu_info.name}")
        print(f"GPU Memory: {gpu_info.total_memory / 1024**3:.1f} GB")
    else:
        raise RuntimeError("No GPU available in Colab!")
    
    # è®¾ç½®é¡¹ç›®è·¯å¾„
    project_root = Path('/content/ASD_AU_Project')
    if not project_root.exists():
        raise RuntimeError(f"Project not found at {project_root}. Please clone the repository first!")
    
    # æ·»åŠ è·¯å¾„
    sys.path.append(str(project_root))
    sys.path.append(str(project_root / 'code' / 'yolov9'))
    
    # åˆ›å»ºå¿…è¦çš„ç›®å½•
    drive_save_dir = Path('/content/drive/MyDrive/ASD_AU_weights/stage1')
    drive_save_dir.mkdir(parents=True, exist_ok=True)
    
    # æ£€æŸ¥æ•°æ®åˆ’åˆ†è„šæœ¬çš„è¾“å‡ºï¼ˆæ”¯æŒåµŒå¥—è·¯å¾„ï¼‰
    data_candidates = [
        Path('/content/yolo_data/yolo_data'),  # ç”¨æˆ·å½“å‰çš„å®é™…è·¯å¾„
        Path('/content/yolo_data'),            # æ ‡å‡†è·¯å¾„
    ]
    
    found_data = False
    for yolo_data_path in data_candidates:
        if yolo_data_path.exists() and (yolo_data_path / 'images').exists():
            print(f"âœ… Found dataset at: {yolo_data_path}")
            # æ˜¾ç¤ºæ•°æ®ç»Ÿè®¡
            for split in ['train', 'val', 'test']:
                img_dir = yolo_data_path / 'images' / split
                if img_dir.exists():
                    img_count = len(list(img_dir.glob('*')))
                    print(f"   {split:5s}: {img_count:,} images")
            found_data = True
            break
    
    if not found_data:
        print("âš ï¸  No dataset found in expected locations")
    
    return project_root, drive_save_dir

# è®¾ç½®ç¯å¢ƒ
PROJECT_ROOT, DRIVE_SAVE_DIR = setup_colab_env()

# ç¡®ä¿YOLOv9è·¯å¾„æ­£ç¡®æ·»åŠ 
yolov9_path = PROJECT_ROOT / 'code' / 'yolov9'
if str(yolov9_path) not in sys.path:
    sys.path.insert(0, str(yolov9_path))

# éªŒè¯è·¯å¾„å’Œå¯¼å…¥
print(f"YOLOv9 path: {yolov9_path}")
print(f"YOLOv9 path exists: {yolov9_path.exists()}")

try:
    # å¯¼å…¥YOLOv9ç»„ä»¶
    from models.yolo import Model as Yolov9
    from utils.dataloaders import create_dataloader
    from utils.general import increment_path, colorstr, check_img_size, LOGGER
    from utils.loss_tal import ComputeLoss
    from utils.torch_utils import ModelEMA, de_parallel
    from utils.metrics import fitness
    from val import run as validate
    from utils.callbacks import Callbacks
    print("âœ… YOLOv9 modules imported successfully")
except ImportError as e:
    print(f"âŒ Failed to import YOLOv9 modules: {e}")
    print("ğŸ“ Available files in yolov9 directory:")
    if yolov9_path.exists():
        import os
        for item in os.listdir(yolov9_path):
            print(f"   {item}")
    raise

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from yolo_modules.model_wrapper import Y9_GAT_DA


class ColabStage1Trainer:
    def __init__(self, config):
        self.config = config
        self.device = torch.device(config['device'])
        
        # ä½¿ç”¨ä¸´æ—¶ç›®å½•ä½œä¸ºå·¥ä½œç›®å½•
        self.save_dir = Path(config['save_dir'])
        self.save_dir.mkdir(parents=True, exist_ok=True)
        
        # Driveä¿å­˜ç›®å½•
        self.drive_save_dir = DRIVE_SAVE_DIR
        
        # è®¾ç½®æ—¥å¿—
        self.setup_logging()
        
        # åˆå§‹åŒ–å›è°ƒ
        self.callbacks = Callbacks()
        
        # Colabç‰¹å®šè®¾ç½®
        self.setup_colab_specific()
        
    def setup_colab_specific(self):
        """Colabç‰¹å®šçš„è®¾ç½®"""
        # å‡å°‘æ˜¾å­˜ç¢ç‰‡
        torch.cuda.empty_cache()
        gc.collect()
        
        # è®¾ç½®cudnn
        torch.backends.cudnn.benchmark = True
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ä¹‹å‰çš„checkpoint
        self.check_resume()
        
    def check_resume(self):
        """æ£€æŸ¥æ˜¯å¦éœ€è¦æ¢å¤è®­ç»ƒ"""
        checkpoint_pattern = self.drive_save_dir / 'checkpoint_epoch_*.pt'
        checkpoints = list(self.drive_save_dir.glob('checkpoint_epoch_*.pt'))
        
        if checkpoints:
            # æ‰¾åˆ°æœ€æ–°çš„checkpoint
            latest_checkpoint = max(checkpoints, key=lambda p: int(p.stem.split('_')[-1]))
            self.logger.info(f"Found checkpoint: {latest_checkpoint}")
            self.config['resume'] = str(latest_checkpoint)
        else:
            self.config['resume'] = None
    
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ"""
        # åŒæ—¶ä¿å­˜åˆ°æœ¬åœ°å’ŒDrive
        log_file = self.save_dir / f"train_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
        drive_log_file = self.drive_save_dir / f"train_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file),
                logging.FileHandler(drive_log_file),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
        
    def create_model(self):
        """åˆ›å»ºYOLOv9æ£€æµ‹å™¨æ¨¡å‹ï¼ˆStage 1 åªè®­ç»ƒæ£€æµ‹å™¨ï¼‰"""
        self.logger.info("Creating YOLOv9 detector for Stage 1...")
        
        # ç›´æ¥åˆ›å»ºYOLOv9æ£€æµ‹å™¨
        cfg_path = self.config['model_cfg']
        
        # å¦‚æœæ˜¯ç›¸å¯¹è·¯å¾„ï¼Œå°è¯•å¤šä¸ªä½ç½®æŸ¥æ‰¾
        if not os.path.isabs(cfg_path):
            cfg_candidates = [
                os.path.join('/content', cfg_path),
                os.path.join(PROJECT_ROOT, 'code', 'yolov9', 'models', 'detect', cfg_path),
                os.path.join(PROJECT_ROOT, 'code', 'yolov9', 'models', cfg_path),
                os.path.join(PROJECT_ROOT, 'code', 'yolov9', 'models', 'detect', 'yolov9-c.yaml'),  # å›é€€
            ]
            
            cfg_found = None
            for candidate in cfg_candidates:
                if os.path.exists(candidate):
                    cfg_found = candidate
                    self.logger.info(f"Found config at: {cfg_found}")
                    break
            
            if cfg_found is None:
                raise FileNotFoundError(f"Config file not found. Tried: {cfg_candidates}")
            cfg_path = cfg_found
        
        # åˆ›å»ºYOLOv9æ¨¡å‹
        model = Yolov9(cfg=cfg_path, ch=3, nc=1)  # nc=1 for face detection
        
        # åŠ è½½é¢„è®­ç»ƒæƒé‡
        if self.config['pretrained_weights']:
            weights_path = Path(self.config['pretrained_weights'])
            
            # æŒ‰ä¼˜å…ˆçº§æŸ¥æ‰¾æƒé‡æ–‡ä»¶
            if not weights_path.exists():
                # æ£€æŸ¥å¤šä¸ªå¯èƒ½çš„ä½ç½®
                weight_candidates = [
                    Path('/content') / weights_path.name,                                    # Colabæ ¹ç›®å½•
                    Path('/content/drive/MyDrive/ASD_AU_weights/pretrained') / weights_path.name,  # Driveä½ç½®
                    self.drive_save_dir.parent / 'pretrained' / weights_path.name,          # é»˜è®¤Driveä½ç½®
                    Path('/content') / 'yolov9c_face_weights_only.pt',                      # ä¿®æ­£çš„æƒé‡æ–‡ä»¶ï¼ˆç›´æ¥è·¯å¾„ï¼‰
                ]
                
                for candidate in weight_candidates:
                    if candidate.exists():
                        weights_path = candidate
                        break
            
            if weights_path.exists():
                self.logger.info(f"Loading pretrained weights from {weights_path}")
                
                # æ£€æŸ¥æ˜¯å¦ä¸ºultralyticsæ ¼å¼çš„æƒé‡
                try:
                    # å°è¯•ç›´æ¥åŠ è½½PyTorchæƒé‡
                    ckpt = torch.load(weights_path, map_location=self.device, weights_only=False)
                except Exception as e1:
                    self.logger.warning(f"Direct PyTorch loading failed: {e1}")
                    
                    # å°è¯•ä½¿ç”¨ultralyticsåŠ è½½å¹¶è½¬æ¢
                    try:
                        self.logger.info("Attempting to load ultralytics format and convert...")
                        import subprocess
                        import sys
                        
                        # å®‰è£…ultralyticså¦‚æœæ²¡æœ‰
                        try:
                            import ultralytics
                        except ImportError:
                            self.logger.info("Installing ultralytics...")
                            subprocess.check_call([sys.executable, "-m", "pip", "install", "ultralytics"])
                            import ultralytics
                        
                        from ultralytics import YOLO
                        
                        # åŠ è½½ultralyticsæ¨¡å‹
                        yolo_model = YOLO(str(weights_path))
                        
                        # æå–çº¯PyTorchæƒé‡
                        pure_weights = yolo_model.model.state_dict()
                        
                        # ä¿å­˜è½¬æ¢åçš„æƒé‡
                        converted_path = weights_path.parent / f"{weights_path.stem}_converted.pt"
                        torch.save(pure_weights, converted_path)
                        self.logger.info(f"Converted weights saved to: {converted_path}")
                        
                        # åˆ›å»ºckptå­—å…¸æ ¼å¼
                        ckpt = {'model': pure_weights}
                        
                    except Exception as e2:
                        self.logger.error(f"Ultralytics conversion failed: {e2}")
                        self.logger.warning("Proceeding without pretrained weights...")
                        ckpt = None
                
                # åŠ è½½æ£€æµ‹å™¨æƒé‡ï¼ˆç›´æ¥åŠ è½½åˆ°YOLOv9æ¨¡å‹ï¼‰
                if ckpt is not None:
                    state_dict = ckpt['model'] if 'model' in ckpt else ckpt
                    if hasattr(state_dict, 'state_dict'):
                        state_dict = state_dict.state_dict()
                    
                    # è·å–æ¨¡å‹å½“å‰çš„state_dictä½œä¸ºå‚è€ƒ
                    model_state = model.state_dict()
                    compatible_state = {}
                    
                    # å°è¯•ç›´æ¥åŒ¹é…é”®å
                    for k, v in state_dict.items():
                        # å»æ‰å¯èƒ½çš„detectorå‰ç¼€
                        clean_k = k.replace('detector.', '') if k.startswith('detector.') else k
                        if clean_k in model_state and v.shape == model_state[clean_k].shape:
                            compatible_state[clean_k] = v
                    
                    # åŠ è½½æƒé‡
                    if compatible_state:
                        missing_keys, unexpected_keys = model.load_state_dict(compatible_state, strict=False)
                        self.logger.info(f"Pretrained weights loaded successfully. Matched {len(compatible_state)} layers.")
                        
                        # è¯¦ç»†çš„åŠ è½½ä¿¡æ¯
                        if missing_keys:
                            self.logger.info(f"Missing keys (will be randomly initialized): {len(missing_keys)}")
                        if unexpected_keys:
                            self.logger.info(f"Unexpected keys (ignored): {len(unexpected_keys)}")
                    else:
                        self.logger.warning("No compatible weights found! Training from scratch...")
                        
                    # æ˜¾ç¤ºåŒ¹é…ç»Ÿè®¡
                    total_model_params = len(model_state.keys())
                    self.logger.info(f"Weight loading: {len(compatible_state)}/{total_model_params} layers matched")
                else:
                    self.logger.warning("No pretrained weights loaded, training from scratch")
            else:
                self.logger.warning(f"Pretrained weights not found: {weights_path}")
        
        # æ¢å¤è®­ç»ƒï¼ˆè·³è¿‡ä¸å…¼å®¹çš„checkpointï¼‰
        start_epoch = 0
        if self.config.get('resume'):
            self.logger.info(f"Found checkpoint: {self.config['resume']}")
            try:
                checkpoint = torch.load(self.config['resume'], map_location=self.device)
                # æ£€æŸ¥checkpointæ˜¯å¦ä¸å½“å‰æ¨¡å‹æ¶æ„å…¼å®¹
                checkpoint_keys = set(checkpoint['model_state_dict'].keys())
                model_keys = set(model.state_dict().keys())
                
                # å¦‚æœcheckpointåŒ…å«GAT/emotionç»„ä»¶ï¼Œè¯´æ˜æ˜¯æ—§çš„åŒ…è£…æ¨¡å‹ï¼Œè·³è¿‡
                has_old_components = any(key.startswith(('gat_head', 'emotion_head', 'roi_extractor')) 
                                       for key in checkpoint_keys)
                
                if has_old_components:
                    self.logger.warning("Checkpoint contains old model architecture (Y9_GAT_DA). Starting fresh training with new YOLOv9-only architecture.")
                    start_epoch = 0
                else:
                    # å…¼å®¹çš„checkpointï¼Œå¯ä»¥æ¢å¤
                    model.load_state_dict(checkpoint['model_state_dict'], strict=False)
                    start_epoch = checkpoint['epoch'] + 1
                    self.logger.info(f"Resumed from epoch {start_epoch}")
                    
            except Exception as e:
                self.logger.warning(f"Failed to load checkpoint: {e}. Starting fresh training.")
                start_epoch = 0
        
        # Stage 1ä¸éœ€è¦å†»ç»“æ¨¡å—ï¼ˆåªæœ‰æ£€æµ‹å™¨ï¼‰
        # æ‰“å°å‚æ•°ç»Ÿè®¡
        self.print_param_stats(model)
        
        return model, start_epoch
    
    def freeze_modules(self, model):
        """å†»ç»“GAT-AUå¤´å’Œæƒ…ç»ªå¤´çš„å‚æ•°"""
        frozen_modules = ['gat_head', 'emotion_head', 'roi_extractor']
        
        for name, param in model.named_parameters():
            if any(module in name for module in frozen_modules):
                param.requires_grad = False
                
        self.logger.info(f"Frozen modules: {frozen_modules}")
        
    def print_param_stats(self, model):
        """æ‰“å°å‚æ•°ç»Ÿè®¡"""
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        
        self.logger.info(f"Total parameters: {total_params:,}")
        self.logger.info(f"Trainable parameters: {trainable_params:,}")
        self.logger.info(f"Frozen parameters: {total_params - trainable_params:,}")
        
    def setup_data_paths(self):
        """è®¾ç½®æ•°æ®è·¯å¾„ï¼ˆå…¼å®¹ç”¨æˆ·çš„æ•°æ®åˆ’åˆ†è„šæœ¬ï¼‰"""
        # æŒ‰ä¼˜å…ˆçº§æ£€æŸ¥æ•°æ®è·¯å¾„
        data_candidates = [
            Path('/content/yolo_data/yolo_data'),                         # ç”¨æˆ·å½“å‰çš„å®é™…è·¯å¾„
            Path('/content/yolo_data'),                                    # ç”¨æˆ·åˆ’åˆ†è„šæœ¬çš„æ ‡å‡†è¾“å‡º
            Path('/content/drive/MyDrive/datasets/HDA-SynChild'),         # Driveæ ‡å‡†ä½ç½®
            Path('/content/datasets/HDA-SynChild'),                       # æœ¬åœ°ä½ç½®
        ]
        
        for data_root in data_candidates:
            if data_root.exists() and (data_root / 'images' / 'train').exists():
                self.logger.info(f"Found dataset at: {data_root}")
                
                # å¦‚æœæ•°æ®åœ¨åµŒå¥—ä½ç½®ï¼Œåˆ›å»ºç¬¦å·é“¾æ¥åˆ°æ ‡å‡†ä½ç½®ä¼˜åŒ–è®¿é—®
                if str(data_root) in ['/content/yolo_data', '/content/yolo_data/yolo_data']:
                    standard_path = Path('/content/datasets/HDA-SynChild')
                    standard_path.parent.mkdir(exist_ok=True)
                    if not standard_path.exists():
                        os.symlink(data_root, standard_path)
                        self.logger.info(f"Created symlink: {standard_path} -> {data_root}")
                        return standard_path
                
                return data_root
        
        # å¦‚æœéƒ½æ²¡æ‰¾åˆ°ï¼Œç»™å‡ºè¯¦ç»†çš„æç¤º
        raise RuntimeError(
            "Dataset not found! Please ensure your data is in one of these locations:\n"
            "1. /content/yolo_data (output from your data split script)\n"
            "2. /content/drive/MyDrive/datasets/HDA-SynChild\n"
            "3. /content/datasets/HDA-SynChild\n"
            "Required structure: {path}/images/train/, {path}/images/val/, {path}/labels/train/, {path}/labels/val/"
        )
    
    def create_dataloaders(self):
        """åˆ›å»ºæ•°æ®åŠ è½½å™¨ï¼ˆColabä¼˜åŒ–ï¼‰"""
        # è®¾ç½®æ•°æ®è·¯å¾„
        data_root = self.setup_data_paths()
        
        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨ç”¨æˆ·ç”Ÿæˆçš„data.yaml
        user_data_yaml = data_root / 'data.yaml'
        if user_data_yaml.exists():
            self.logger.info(f"Using existing data.yaml from: {user_data_yaml}")
            data_yaml = user_data_yaml
        else:
            # åˆ›å»ºdata yaml
            data_yaml = self.save_dir / 'hda_synchild_colab.yaml'
            data_dict = {
                'path': str(data_root),
                'train': 'images/train',
                'val': 'images/val',
                'names': {0: 'face'}
            }
            
            with open(data_yaml, 'w') as f:
                yaml.dump(data_dict, f)
            self.logger.info(f"Created data.yaml at: {data_yaml}")
        
        
        # Colabä¼˜åŒ–çš„è¶…å‚æ•°
        hyp = self.get_hyp_dict()
        
        # æ ¹æ®GPUå†…å­˜è°ƒæ•´batch size
        gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1024**3
        if gpu_mem < 16:  # T4 GPU (15GB)
            self.config['batch_size'] = min(self.config['batch_size'], 8)
            self.logger.info(f"Adjusted batch size to {self.config['batch_size']} for GPU memory")
        
        # è®­ç»ƒæ•°æ®åŠ è½½å™¨
        train_loader, dataset = create_dataloader(
            path=data_root / 'images/train',
            imgsz=self.config['img_size'],
            batch_size=self.config['batch_size'],
            stride=32,
            single_cls=True,
            hyp=hyp,
            augment=True,
            cache='ram' if gpu_mem > 20 else False,  # åªåœ¨å¤§å†…å­˜æ—¶ç¼“å­˜
            rect=False,
            rank=-1,
            workers=2,  # Colab CPUé™åˆ¶
            image_weights=False,
            quad=False,
            prefix='train: ',
            shuffle=True
        )
        
        # éªŒè¯æ•°æ®åŠ è½½å™¨
        val_loader = create_dataloader(
            path=data_root / 'images/val',
            imgsz=self.config['img_size'],
            batch_size=self.config['batch_size'] * 2,
            stride=32,
            single_cls=True,
            hyp=hyp,
            augment=False,
            cache='ram' if gpu_mem > 20 else False,
            rect=True,
            rank=-1,
            workers=2,
            pad=0.5,
            prefix='val: '
        )[0]
        
        return train_loader, val_loader, dataset
    
    def get_hyp_dict(self):
        """è·å–æ•°æ®å¢å¼ºè¶…å‚æ•°ï¼ˆColabä¼˜åŒ–ï¼‰"""
        return {
            'lr0': self.config['lr0'],
            'lrf': 0.1,
            'momentum': 0.937,
            'weight_decay': 0.0005,
            'warmup_epochs': 3.0,
            'warmup_momentum': 0.8,
            'warmup_bias_lr': 0.1,
            'box': 0.05,
            'cls': 0.5,
            'cls_pw': 1.0,
            'obj': 1.0,
            'obj_pw': 1.0,
            'iou_t': 0.20,
            'anchor_t': 4.0,
            'anchors': 3,
            'fl_gamma': 0.0,
            'hsv_h': 0.015,
            'hsv_s': 0.7,
            'hsv_v': 0.4,
            'degrees': 0.0,
            'translate': 0.1,
            'scale': 0.5,
            'shear': 0.0,
            'perspective': 0.0,
            'flipud': 0.0,
            'fliplr': 0.5,
            'mosaic': 0.5,  # é™ä½ä»¥èŠ‚çœå†…å­˜
            'mixup': 0.0,
            'copy_paste': 0.0,
            'paste_in': 0.15,
            'loss_ota': 1,
        }
    
    def save_checkpoint(self, model, optimizer, epoch, best_fitness, ema=None, best=False):
        """ä¿å­˜æ£€æŸ¥ç‚¹åˆ°Google Drive"""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'best_fitness': best_fitness,
            'config': self.config
        }
        
        if ema:
            checkpoint['ema_state_dict'] = ema.ema.state_dict()
        
        # ä¿å­˜åˆ°æœ¬åœ°
        if best:
            local_path = self.save_dir / 'weights' / 'best.pt'
        else:
            local_path = self.save_dir / 'weights' / f'epoch_{epoch}.pt'
        
        local_path.parent.mkdir(exist_ok=True)
        torch.save(checkpoint, local_path)
        
        # å¤åˆ¶åˆ°Drive
        if best:
            drive_path = self.drive_save_dir / 'best.pt'
        else:
            drive_path = self.drive_save_dir / f'checkpoint_epoch_{epoch}.pt'
            # åªä¿ç•™æœ€è¿‘5ä¸ªcheckpoint
            self.cleanup_old_checkpoints()
        
        shutil.copy2(local_path, drive_path)
        self.logger.info(f"Checkpoint saved to {drive_path}")
    
    def cleanup_old_checkpoints(self):
        """æ¸…ç†æ—§çš„æ£€æŸ¥ç‚¹ï¼Œåªä¿ç•™æœ€è¿‘çš„5ä¸ª"""
        checkpoints = list(self.drive_save_dir.glob('checkpoint_epoch_*.pt'))
        if len(checkpoints) > 5:
            checkpoints.sort(key=lambda p: int(p.stem.split('_')[-1]))
            for ckpt in checkpoints[:-5]:
                ckpt.unlink()
                self.logger.info(f"Removed old checkpoint: {ckpt}")
    
    def train_epoch(self, model, train_loader, optimizer, compute_loss, epoch, epochs):
        """è®­ç»ƒä¸€ä¸ªepochï¼ˆå†…å­˜ä¼˜åŒ–ï¼‰"""
        model.train()
        pbar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f'Epoch {epoch+1}/{epochs}')
        
        losses = []
        accumulation_steps = self.config.get('gradient_accumulation', 1)
        
        for i, (imgs, targets, paths, _) in pbar:
            imgs = imgs.to(self.device, non_blocking=True).float() / 255.0
            
            # å‰å‘ä¼ æ’­
            with torch.amp.autocast('cuda', enabled=self.config.get('amp', True)):
                # ç›´æ¥ä½¿ç”¨YOLOv9æ¨¡å‹
                pred = model(imgs)
                
                # ç¬¬ä¸€ä¸ªbatchæ—¶æ‰“å°è°ƒè¯•ä¿¡æ¯
                if i == 0:
                    self.logger.info(f"Raw model output type: {type(pred)}")
                    self.logger.info(f"Model training mode: {model.training}")
                    
                loss, loss_items = compute_loss(pred, targets.to(self.device))
                loss = loss / accumulation_steps
            
            # åå‘ä¼ æ’­
            loss.backward()
            
            # æ¢¯åº¦ç´¯ç§¯
            if (i + 1) % accumulation_steps == 0:
                optimizer.step()
                optimizer.zero_grad()
            
            # æ›´æ–°è¿›åº¦æ¡
            losses.append(loss_items.cpu().numpy())
            if len(losses) > 100:
                losses = losses[-100:]  # é™åˆ¶å†…å­˜ä½¿ç”¨
            
            mean_loss = np.mean(losses, axis=0)
            pbar.set_postfix({
                'loss': f'{mean_loss[0]:.4f}',
                'GPU': f'{torch.cuda.memory_reserved() / 1024**3:.1f}G'
            })
            
            # å®šæœŸæ¸…ç†æ˜¾å­˜
            if i % 100 == 0:
                torch.cuda.empty_cache()
        
        return np.mean(losses, axis=0)
    
    def train(self):
        """ä¸»è®­ç»ƒå‡½æ•°"""
        # åˆ›å»ºæ¨¡å‹
        model, start_epoch = self.create_model()
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader, val_loader, dataset = self.create_dataloaders()
        
        # æ·»åŠ è¶…å‚æ•°åˆ°æ¨¡å‹ï¼ˆæŸå¤±å‡½æ•°éœ€è¦ï¼‰
        hyp = self.get_hyp_dict()
        model.hyp = hyp
        
        # è®¾ç½®æ¨¡å‹ä¸ºè®­ç»ƒæ¨¡å¼
        model.train()
        
        # ç¡®ä¿æ¨¡å‹strideå’Œæ£€æµ‹å¤´æ­£ç¡®åˆå§‹åŒ–
        self.logger.info("Initializing model detection head...")
        with torch.no_grad():
            _ = model(torch.zeros(1, 3, self.config['img_size'], self.config['img_size'], device=self.device))
        
        # åˆ›å»ºæŸå¤±å‡½æ•°
        compute_loss = ComputeLoss(model)
        
        # åˆ›å»ºä¼˜åŒ–å™¨
        optimizer = self.create_optimizer(model)
        
        # æ¢å¤ä¼˜åŒ–å™¨çŠ¶æ€
        if self.config.get('resume') and start_epoch > 0:
            checkpoint = torch.load(self.config['resume'], map_location=self.device)
            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        
        # åˆ›å»ºè°ƒåº¦å™¨
        scheduler = self.create_scheduler(optimizer, self.config['epochs'] - start_epoch)
        
        # åˆ›å»ºEMA
        ema = ModelEMA(model) if self.config.get('use_ema', True) else None
        
        # è®­ç»ƒå¾ªç¯
        best_fitness = 0
        patience_counter = 0
        patience = self.config.get('patience', 8)
        
        try:
            for epoch in range(start_epoch, self.config['epochs']):
                # è®­ç»ƒ
                train_loss = self.train_epoch(model, train_loader, optimizer, compute_loss, 
                                            epoch, self.config['epochs'])
                
                # éªŒè¯
                val_loss = self.validate(model, val_loader, compute_loss)
                
                # æ›´æ–°EMA
                if ema:
                    ema.update(model)
                
                # æ›´æ–°å­¦ä¹ ç‡
                scheduler.step()
                
                # è®°å½•æ—¥å¿—
                self.logger.info(
                    f"Epoch {epoch+1}/{self.config['epochs']} - "
                    f"Train Loss: {train_loss[0]:.4f} - "
                    f"Val Loss: {val_loss[0]:.4f} - "
                    f"LR: {optimizer.param_groups[0]['lr']:.6f} - "
                    f"Patience: {patience_counter}/{patience}"
                )
                
                # ä¿å­˜æ£€æŸ¥ç‚¹
                if (epoch + 1) % self.config['save_period'] == 0:
                    self.save_checkpoint(model, optimizer, epoch, best_fitness, ema)
                
                # ä¿å­˜æœ€ä½³æ¨¡å‹å’Œæ—©åœæ£€æŸ¥
                fitness = 1 / (val_loss[0] + 1e-6)
                if fitness > best_fitness:
                    best_fitness = fitness
                    patience_counter = 0
                    self.save_checkpoint(model, optimizer, epoch, best_fitness, ema, best=True)
                    self.logger.info(f"New best model! Fitness: {fitness:.6f}")
                else:
                    patience_counter += 1
                    
                # æ—©åœæ£€æŸ¥
                if patience_counter >= patience:
                    self.logger.info(f"Early stopping triggered at epoch {epoch+1}. "
                                   f"No improvement for {patience} epochs.")
                    self.save_checkpoint(model, optimizer, epoch, best_fitness, ema)
                    break
                
                # æ£€æŸ¥Colabè¿è¡Œæ—¶é—´
                if self.check_runtime_limit():
                    self.logger.warning("Approaching Colab runtime limit. Saving checkpoint...")
                    self.save_checkpoint(model, optimizer, epoch, best_fitness, ema)
                    break
                    
        except Exception as e:
            self.logger.error(f"Training interrupted: {e}")
            self.save_checkpoint(model, optimizer, epoch, best_fitness, ema)
            raise
        
        self.logger.info("Training completed!")
        return str(self.drive_save_dir / 'best.pt')
    
    def check_runtime_limit(self):
        """æ£€æŸ¥æ˜¯å¦æ¥è¿‘Colabè¿è¡Œæ—¶é™åˆ¶"""
        # è¿™é‡Œå¯ä»¥å®ç°æ›´å¤æ‚çš„é€»è¾‘
        # ç®€å•èµ·è§ï¼Œè®­ç»ƒè¶…è¿‡11å°æ—¶å°±åœæ­¢
        import time
        if hasattr(self, 'start_time'):
            elapsed = time.time() - self.start_time
            return elapsed > 11 * 3600  # 11å°æ—¶
        return False
    
    def create_optimizer(self, model):
        """åˆ›å»ºä¼˜åŒ–å™¨"""
        params_to_optimize = [p for p in model.parameters() if p.requires_grad]
        
        if self.config['optimizer'] == 'AdamW':
            optimizer = torch.optim.AdamW(
                params_to_optimize,
                lr=self.config['lr0'],
                betas=(0.937, 0.999),
                weight_decay=0.0005
            )
        else:
            optimizer = torch.optim.SGD(
                params_to_optimize,
                lr=self.config['lr0'],
                momentum=0.937,
                weight_decay=0.0005,
                nesterov=True
            )
        
        return optimizer
    
    def create_scheduler(self, optimizer, epochs):
        """åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨"""
        if self.config['scheduler'] == 'cosine':
            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
                optimizer,
                T_max=epochs,
                eta_min=self.config['lr0'] * 0.1
            )
        else:
            lambda_lr = lambda epoch: (1 - epoch / epochs) * (1 - 0.1) + 0.1
            scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_lr)
        
        return scheduler
    
    def validate(self, model, val_loader, compute_loss):
        """éªŒè¯æ¨¡å‹"""
        model.eval()
        losses = []
        
        with torch.no_grad():
            for imgs, targets, paths, _ in tqdm(val_loader, desc='Validating'):
                imgs = imgs.to(self.device, non_blocking=True).float() / 255.0
                
                with torch.amp.autocast('cuda', enabled=self.config.get('amp', True)):
                    pred = model(imgs)
                    loss, loss_items = compute_loss(pred, targets.to(self.device))
                
                losses.append(loss_items.cpu().numpy())
        
        return np.mean(losses, axis=0)


def main():
    """ä¸»å‡½æ•° - Colabç‰ˆæœ¬"""
    import time
    
    config = {
        # æ¨¡å‹é…ç½®
        'model_cfg': 'yolov9c-face-exact.yaml',  # ä¼˜å…ˆä½¿ç”¨ä¿®æ­£çš„é…ç½®ï¼Œå›é€€åˆ°æ ‡å‡†é…ç½®
        'pretrained_weights': 'yolov9c_face_weights_only.pt',  # ä¼šè‡ªåŠ¨åœ¨å¤šä¸ªä½ç½®æŸ¥æ‰¾
        'num_au': 32,
        'num_emotion': 6,
        
        # æ•°æ®é…ç½®
        'img_size': 640,
        'batch_size': 8,  # Colab T4 GPUä¼˜åŒ–
        'workers': 2,     # Colab CPUé™åˆ¶
        'cache_images': False,  # èŠ‚çœå†…å­˜
        
        # è®­ç»ƒé…ç½®
        'epochs': 40,
        'lr0': 0.002,
        'optimizer': 'AdamW',
        'scheduler': 'cosine',
        'device': 'cuda:0',
        'use_ema': True,
        'save_period': 5,  # æ›´é¢‘ç¹ä¿å­˜
        'amp': True,  # æ··åˆç²¾åº¦è®­ç»ƒ
        'gradient_accumulation': 2,  # æ¢¯åº¦ç´¯ç§¯
        'patience': 8,  # æ—©åœæœºåˆ¶ï¼š8ä¸ªepochéªŒè¯æŸå¤±ä¸ä¸‹é™å°±åœæ­¢
        
        # ä¿å­˜é…ç½®
        'save_dir': '/content/runs/stage1_hda_synchild',
    }
    
    # è®°å½•å¼€å§‹æ—¶é—´
    start_time = time.time()
    
    # åˆ›å»ºè®­ç»ƒå™¨å¹¶å¼€å§‹è®­ç»ƒ
    trainer = ColabStage1Trainer(config)
    trainer.start_time = start_time
    
    best_weights = trainer.train()
    
    print(f"Training completed! Best weights saved at: {best_weights}")
    print(f"Total time: {(time.time() - start_time) / 3600:.2f} hours")
    

if __name__ == '__main__':
    main()